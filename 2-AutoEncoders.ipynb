{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you missed it, take a quick glance at the previous notebook \"1 - Data Browser\" since it will give you an overview of the datasets we'll use in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At it's core, an autoencoder is a function that takes complex inputs and performs a dimensionality reduction of the data.  Principal Component Analysis is an example of a classical encoding algorithm.  Even a simple linear regression in 2D can serve as an encoder, since you map 2 dimensions (x and y) into just one dimension (position along a line).\n",
    "\n",
    "An important feature of autoencoders is the ability to reconstruct the input data from the compressed feature representation.  It's easy to see how this is done when you have used PCA or linear regression, but it is more complex with a neural network.\n",
    "\n",
    "One advantage of using neural networks as auto-encoders is that they have capacity to encode complex data, and the training regime for encoding is not overly complex and similar for many types of data.\n",
    "\n",
    "In this notebook, we will build an autoencoder for handwritten digits between 0 and 99, inclusive, generated via the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data generator and tensorflow:\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "tf.enable_eager_execution()\n",
    "from src.utils import data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Build our models\n",
    "\n",
    "We'll build two autoencoder models, one with a classic neural network and one with a convolutional neural network.  We'll map both down to the same intermediate representation.\n",
    "\n",
    "Our decoder stages will be built on the same core operations as the encoders but in reverse.\n",
    "\n",
    "#### Feed Forward NN encoder (no convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import neural_net_AE\n",
    "\n",
    "# Core training parameters:\n",
    "N_TRAINING_ITERATION = 5000\n",
    "BATCH_SIZE = 64\n",
    "NUM_DIGITS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up our models:\n",
    "\n",
    "encoder = neural_net_AE.Encoder()\n",
    "decoder = neural_net_AE.Decoder(NUM_DIGITS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using eager execution, we can not view the models until we run through them at least once.  So let's set up the data generator and then we can see the layers of the network clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = data_generator.mnist_generator()\n",
    "\n",
    "# Load some data:\n",
    "batch_images, batch_labels = data_gen.next_train_batch(BATCH_SIZE, NUM_DIGITS)\n",
    "# Reshape the data:\n",
    "batch_images = batch_images.reshape([BATCH_SIZE, 28*28*NUM_DIGITS])\n",
    "\n",
    "intermediate_state = encoder(batch_images)\n",
    "decoded_images = decoder(intermediate_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the encoder model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  307720    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  38514     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  990       \n",
      "=================================================================\n",
      "Total params: 962,664\n",
      "Trainable params: 962,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Here is the intermediate representation shape:\n",
      "(64, 10)\n",
      "Here is the decoder model: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              multiple                  1078      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  38808     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  308112    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  615440    \n",
      "=================================================================\n",
      "Total params: 963,438\n",
      "Trainable params: 963,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Here is the decoded images shape:\n",
      "(64, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the encoder model:\")\n",
    "encoder.summary()\n",
    "print(\"Here is the intermediate representation shape:\")\n",
    "print(intermediate_state.shape)\n",
    "print(\"Here is the decoder model: \")\n",
    "decoder.summary()\n",
    "print(\"Here is the decoded images shape:\")\n",
    "print(decoded_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an optimizer, we will use Adam Optimizer:\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network proceeds in a few iterative steps:\n",
    " - Load a batch of data\n",
    " - Feed the data into the network, compute the loss, back prop the gradients (which is all handled automatically by tensorflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/corey.adams/Library/Python/3.6/lib/python/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/corey.adams/Library/Python/3.6/lib/python/site-packages/tensorflow/python/distribute/distribute_lib.py:1595: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Step 0, loss 0.11152058839797974\n",
      "Step 50, loss 0.043106336146593094\n",
      "Step 100, loss 0.03324535861611366\n",
      "Step 150, loss 0.031337399035692215\n",
      "Step 200, loss 0.02976047247648239\n",
      "Step 250, loss 0.024238061159849167\n",
      "Step 300, loss 0.02562219835817814\n",
      "Step 350, loss 0.024897152557969093\n",
      "Step 400, loss 0.025628726929426193\n",
      "Step 450, loss 0.025109117850661278\n",
      "Step 500, loss 0.023903602734208107\n",
      "Step 550, loss 0.02372867241501808\n",
      "Step 600, loss 0.023482216522097588\n",
      "Step 650, loss 0.02184278704226017\n",
      "Step 700, loss 0.022857794538140297\n",
      "Step 750, loss 0.02238587476313114\n",
      "Step 800, loss 0.020547810941934586\n",
      "Step 850, loss 0.020007699728012085\n",
      "Step 900, loss 0.01933235302567482\n",
      "Step 950, loss 0.02275266870856285\n",
      "Step 1000, loss 0.019361354410648346\n",
      "Step 1050, loss 0.021235007792711258\n",
      "Step 1100, loss 0.018926920369267464\n",
      "Step 1150, loss 0.019148211926221848\n",
      "Step 1200, loss 0.01858360506594181\n",
      "Step 1250, loss 0.018501052632927895\n",
      "Step 1300, loss 0.018135573714971542\n",
      "Step 1350, loss 0.019359400495886803\n",
      "Step 1400, loss 0.017216350883245468\n",
      "Step 1450, loss 0.019001370295882225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2aac778631f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     optimizer.apply_gradients(zip(grads, trainable_vars),\n\u001b[0;32m---> 37\u001b[0;31m                              global_step=tf.train.get_or_create_global_step())\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconverted_grads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta1_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beta1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta2_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beta2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m   \"\"\"\n\u001b[0;32m-> 1039\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                          as_ref=False):\n\u001b[1;32m    303\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    243\u001b[0m   \"\"\"\n\u001b[1;32m    244\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 245\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;34m\"\"\"Implementation of constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_gen = data_generator.mnist_generator()\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "val_steps = []\n",
    "\n",
    "for i in range(N_TRAINING_ITERATION):\n",
    "\n",
    "    # Load some data:\n",
    "    batch_images, batch_labels = data_gen.next_train_batch(BATCH_SIZE, NUM_DIGITS)\n",
    "    # Reshape the data:\n",
    "    batch_images = batch_images.reshape([BATCH_SIZE, 28*28*NUM_DIGITS])\n",
    "    with tf.GradientTape() as tape:\n",
    "        intermediate_state = encoder(batch_images)\n",
    "        decoded_images = decoder(intermediate_state)\n",
    "        loss_value = tf.losses.mean_squared_error(batch_images, decoded_images)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        test_images, test_labels = data_gen.next_test_batch(BATCH_SIZE, NUM_DIGITS)\n",
    "        test_images = test_images.reshape([BATCH_SIZE, 28*28*NUM_DIGITS])\n",
    "        val_intermediate_state = encoder(test_images)\n",
    "        val_decoded_images = decoder(intermediate_state)\n",
    "        val_loss_value = tf.losses.mean_squared_error(batch_images, decoded_images)\n",
    "        val_loss_history.append(val_loss_value.numpy())\n",
    "        val_steps.append(i)\n",
    "\n",
    "\n",
    "        \n",
    "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "\n",
    "    loss_history.append(loss_value.numpy())\n",
    "\n",
    "    # Apply the update to the model:\n",
    "    grads = tape.gradient(loss_value, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_vars),\n",
    "                             global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"Step {}, loss {}\".format(i, loss_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop, 5000 iterations took 2 or 3 minutues to train (on the CPU).  So you should get to good results without too much delay.  Let's plot the loss to see how training progressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "plt.plot(range(len(loss_history)), loss_history, label=\"Train Loss\")\n",
    "plt.plot(val_steps, val_loss_history, label=\"Test Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Training step\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is reasonable behavior for a loss function, the loss decreases rapidly at first and then slows.  Since we didn't evaluate on the test set, it's impossible to know if this model was overtrained or not just from this plot.  But there are other ways to check.\n",
    "\n",
    "Next let's compare the original images, intermediate representations, and output images.  We can evaluate this for both the input and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(_encoder, _decoder, input_images):\n",
    "\n",
    "    N_INFERENCE_IMAGES = input_images.shape[0]\n",
    "    input_images = input_images.reshape(N_INFERENCE_IMAGES, \n",
    "                                        NUM_DIGITS*28*28)\n",
    "\n",
    "\n",
    "    intermediate_rep = _encoder(input_images)\n",
    "    decoded_images = _decoder(intermediate_rep)\n",
    "    decoded_images = decoded_images.numpy().reshape(\n",
    "        N_INFERENCE_IMAGES*28, NUM_DIGITS*28)\n",
    "    \n",
    "    return intermediate_rep, decoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INFERENCE_IMAGES = 2\n",
    "\n",
    "\n",
    "original_images, labels = data_gen.next_train_batch(\n",
    "    N_INFERENCE_IMAGES, NUM_DIGITS)\n",
    "\n",
    "intermediate_rep, decoded_images = run_inference(\n",
    "    encoder, decoder, original_images)\n",
    "original_images = original_images.reshape(N_INFERENCE_IMAGES*28, NUM_DIGITS*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, N_INFERENCE_IMAGES*5))\n",
    "plt.imshow(original_images)\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(5, N_INFERENCE_IMAGES*5))\n",
    "plt.imshow(decoded_images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is interesting!  The outputs of the autoencoder definitely look like numbers, though not *exactly* the numbers that went into the network.  What does the intermediate representation look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(intermediate_rep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell what the numbers are from the intermediate representation?  I certainly can't.  But the decoder can, to a large extent, and that is impressive!\n",
    "\n",
    "Below, we use the autoencoder on the test dataset, which it has never seen before, to see how well the model generalizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INFERENCE_IMAGES = 2\n",
    "\n",
    "\n",
    "original_images, labels = data_gen.next_test_batch(\n",
    "    N_INFERENCE_IMAGES, NUM_DIGITS)\n",
    "\n",
    "intermediate_rep, decoded_images = run_inference(\n",
    "    encoder, decoder, original_images)\n",
    "original_images = original_images.reshape(N_INFERENCE_IMAGES*28, NUM_DIGITS*28)\n",
    "\n",
    "fig = plt.figure(figsize=(5, N_INFERENCE_IMAGES*5))\n",
    "plt.imshow(original_images)\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(5, N_INFERENCE_IMAGES*5))\n",
    "plt.imshow(decoded_images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your models\n",
    "\n",
    "As a last step in this notebook, let's save the trained encoders and decoders.  We will use them later to compare different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights(\"saved_models/user/nn_encoder.h5\")\n",
    "decoder.save_weights(\"saved_models/user/nn_decoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the 3rd notebook in this tutorial, convolutional autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
